================================================================================
WEBARENA EVALUATION SETUP INSTRUCTIONS
RAGEN + A*PO Assignment - WebShop vs WebArena Comparison
================================================================================

This guide will help you:
1. Setup WebArena evaluation environment
2. Run baseline evaluation
3. Generate comparison tables
4. Review updated failure analysis

================================================================================
QUICK START (Recommended - Uses Published Baselines)
================================================================================

This is the FASTEST way to get comparison results without setting up Docker:

Step 1: Run WebArena baseline evaluation (uses published results)
------------------------------------------------------------------------
python eval/evaluate_webarena_baseline.py --mode quick --baseline gpt-4-cot

Expected output:
  ✓ WebArena baseline results loaded (14.41% success rate)
  ✓ Results saved to results/webarena_baseline.json

Step 2: Generate comparison table
------------------------------------------------------------------------
python compare_results.py

Expected output:
  ✓ Loaded WebShop results
  ✓ Loaded WebArena results
  ✓ CSV saved to results/comparison_table.csv
  ✓ Markdown saved to results/comparison_table.md
  ✓ JSON saved to results/comparison_table.json

Step 3: View results
------------------------------------------------------------------------
# View markdown comparison
cat results/comparison_table.md

# View CSV in spreadsheet
open results/comparison_table.csv

# View updated failure analysis
cat results/final_failure_analysis.md

DONE! You now have:
  - Comparison tables (CSV, Markdown, JSON)
  - Updated failure analysis with WebArena section
  - All data needed for your presentation

================================================================================
FULL SETUP (Optional - For Live Evaluation)
================================================================================

Only do this if you want to run ACTUAL WebArena evaluation (slow, complex):

Step 1: Make setup script executable
------------------------------------------------------------------------
chmod +x setup_webarena.sh

Step 2: Run setup script
------------------------------------------------------------------------
./setup_webarena.sh

This will:
  - Clone WebArena repository
  - Check Docker installation
  - Setup Python environment
  - Install dependencies
  - Create .env template

Expected time: 5-10 minutes

Step 3: Configure API key
------------------------------------------------------------------------
nano WebArena/.env

Add your OpenAI API key:
  OPENAI_API_KEY=sk-your-key-here

Save and exit (Ctrl+O, Enter, Ctrl+X)

Step 4: Start Docker containers (OPTIONAL)
------------------------------------------------------------------------
cd WebArena
docker-compose up -d
cd ..

Note: This may take 10-30 minutes and requires ~10GB disk space
You can skip this and still use published baselines!

Step 5: Run live evaluation
------------------------------------------------------------------------
python eval/evaluate_webarena_baseline.py --mode live --num_tasks 50

Note: This will make OpenAI API calls and cost money!
Recommended: Use quick mode instead (see above)

Step 6: Generate comparison
------------------------------------------------------------------------
python compare_results.py

================================================================================
TROUBLESHOOTING
================================================================================

Problem: "WebShop results not found"
Solution:
  The comparison script auto-detects WebShop results from multiple files:
  - results/eval_official.json (preferred)
  - results/evaluation_results.json
  - results/official_minimal_training.json

  If none exist, run WebShop evaluation:
    python eval/evaluate_official.py --num_episodes 100

Problem: "Docker not running"
Solution:
  1. Open Docker Desktop
  2. Wait for it to start (whale icon in menu bar)
  3. Re-run setup script

Problem: "WebArena results not found"
Solution:
  Run WebArena baseline evaluation first:
    python eval/evaluate_webarena_baseline.py --mode quick

Problem: setup_webarena.sh permission denied
Solution:
  chmod +x setup_webarena.sh

================================================================================
OUTPUT FILES
================================================================================

After running the scripts, you will have:

results/webarena_baseline.json
  - WebArena baseline evaluation results
  - Contains: success_rate, avg_steps, agent_type, source

results/comparison_table.csv
  - Spreadsheet-friendly comparison table
  - Columns: Benchmark, Environment, Complexity, Tasks, Success Rate, etc.

results/comparison_table.md
  - Markdown formatted comparison with insights
  - Includes performance metrics, environment details, key insights

results/comparison_table.json
  - JSON format for programmatic access
  - Full comparison data structure

results/final_failure_analysis.md
  - Updated with Section 7: Cross-Benchmark Comparison
  - Detailed analysis of why RAGEN struggles on WebArena
  - Research insights and implications

================================================================================
COMPARISON SUMMARY
================================================================================

Expected Results:

WebShop (RAGEN + A*PO):
  ✓ Success Rate: 67.0%
  ✓ Avg Steps: 9.28
  ✓ Tasks: 100
  ✓ Agent: Trained RAGEN
  ✓ Environment: Simple text-based shopping

WebArena (GPT-4 Baseline):
  ✓ Success Rate: 14.4%
  ✓ Avg Steps: 3.9
  ✓ Tasks: 812
  ✓ Agent: GPT-4 Chain-of-Thought (zero-shot)
  ✓ Environment: Real websites (4 domains)

Key Insight:
  RAGEN achieves 4.7x higher success rate, but on much simpler tasks.
  WebArena is significantly more challenging due to:
    - 1000+ element observation spaces
    - Unlimited compositional actions
    - 30-50 step task horizons
    - Multi-domain environments

================================================================================
FOR YOUR PRESENTATION
================================================================================

Use these files:
1. results/comparison_table.csv - Import into PowerPoint/Keynote
2. results/comparison_table.md - Copy tables into slides
3. results/final_failure_analysis.md - Section 7 has detailed analysis

Key points to present:
  ✓ RAGEN excels on constrained environments (67% on WebShop)
  ✓ WebArena reveals limitations (GPT-4 only reaches 14.4%)
  ✓ Environment complexity matters more than algorithm choice
  ✓ Task-specific training vs zero-shot generalization tradeoff
  ✓ Future work: Hybrid LLM+RL approaches

================================================================================
TIME ESTIMATES
================================================================================

Quick mode (recommended):
  - WebArena evaluation: < 1 second
  - Comparison generation: < 1 second
  - Total time: 5-10 seconds

Full setup mode:
  - Setup script: 5-10 minutes
  - Docker containers: 10-30 minutes (optional)
  - Live evaluation: 1-2 hours (50 tasks, costs $5-10)
  - Not recommended unless you need custom evaluation

================================================================================
QUESTIONS OR ISSUES?
================================================================================

Check these files:
  - This file: WEBARENA_SETUP_INSTRUCTIONS.txt
  - Setup output: Read error messages carefully
  - WebArena docs: WebArena/README.md (after setup)

Common issues:
  - Missing Python packages: pip install -r requirements.txt
  - Permission denied: chmod +x setup_webarena.sh
  - Docker issues: Restart Docker Desktop

================================================================================
END OF INSTRUCTIONS
================================================================================
